{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "# sys.path.insert(1, '../src/utils/')\n",
    "# from helpers import load_json, write_json\n",
    "\n",
    "def load_json(filename):\n",
    "    \"\"\"\n",
    "    Load a JSON file given a filename\n",
    "    If the file doesn't exist, then return an empty dictionary instead\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def write_json(data, filepath):\n",
    "    # assert isinstance(data, dict), '[ERROR] Expect dictionary data!'\n",
    "    json_data = data\n",
    "    if isinstance(data, dict):\n",
    "        json_data = {str(k): data[k] for k in data} # convert deys into str\n",
    "    json_string = json.dumps(json_data, indent = 4)\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        outfile.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curr_dir = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Few-shot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_output = load_json(curr_dir + 'data/output/test_creative_writing_v1_1/inference_output.json')\n",
    "# inference_output\n",
    "ttcw_annotation = load_json(curr_dir + 'data/processed/ttcw/ttcw_annotations.json')\n",
    "claude_output = load_json(curr_dir + 'data/processed/ttcw/claude.json')\n",
    "gpt4_output = load_json(curr_dir + 'data/processed/ttcw/gpt4.json')\n",
    "gpt3_5_output = load_json(curr_dir + 'data/processed/ttcw/gpt3_5.json')\n",
    "\n",
    "len(ttcw_annotation) / 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claude_output[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'story_id': '0_Claude', 'ttcw_idx': 1, 'binary_verdict': 'No'},\n",
       " {'story_id': '0_Claude', 'ttcw_idx': 2, 'binary_verdict': 'No'},\n",
       " {'story_id': '0_Claude', 'ttcw_idx': 3, 'binary_verdict': 'Yes'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttcw_annotation[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_output['0_NewYorker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claude_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(original_data):\n",
    "    verdict2int = {\n",
    "        'Yes': 1,\n",
    "        'No': 0\n",
    "    }\n",
    "    our_data = {}\n",
    "    for original_dp in original_data:\n",
    "        our_dp = {\n",
    "            'prompt_id': original_dp['meta_data']['id'],\n",
    "            'data': original_dp,\n",
    "            'raw_output': original_dp['output']['content'],\n",
    "            'human_output': {k: -1 for k in range(1, 15)}\n",
    "        }\n",
    "        for anno in ttcw_annotation:\n",
    "            if anno['story_id'] == original_dp['meta_data']['id']:\n",
    "                our_dp['human_output'][anno['ttcw_idx']] = verdict2int[anno['binary_verdict']]\n",
    "        our_data[original_dp['meta_data']['id']] = our_dp\n",
    "    return our_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_output_transformed = transform(claude_output)\n",
    "write_json(claude_output_transformed, curr_dir + 'data/output/ttcw_evaluator_check/claude/inference_output.json')\n",
    "\n",
    "gpt3_5_output_transformed = transform(gpt3_5_output)\n",
    "write_json(gpt3_5_output_transformed, curr_dir + 'data/output/ttcw_evaluator_check/gpt3_5/inference_output.json')\n",
    "\n",
    "gpt4_output_transformed = transform(gpt4_output)\n",
    "write_json(gpt4_output_transformed, curr_dir + 'data/output/ttcw_evaluator_check/gpt4/inference_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stories = {}\n",
    "all_stories.update(gpt4_output_transformed)\n",
    "all_stories.update(gpt3_5_output_transformed)\n",
    "all_stories.update(claude_output_transformed)\n",
    "len(list(all_stories.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stories = {k: all_stories[k]['raw_output'] for k in all_stories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_json(all_stories, 'creative_bench/data/processed/ttcw/all_stories.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Sample some few-shot demostrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story_id': '0_Claude', 'ttcw_idx': 1, 'binary_verdict': 'No'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttcw_annotation = load_json(curr_dir + 'data/processed/ttcw/ttcw_annotations.json')\n",
    "ttcw_annotation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = {k: [] for k in range(1, 15)}\n",
    "neg_data = {k: [] for k in range(1, 15)}\n",
    "for dp in ttcw_annotation:\n",
    "    if 'Claude' in dp['story_id'] or 'NewYorker' in dp['story_id']: continue \n",
    "    if dp['binary_verdict'] == 'Yes':\n",
    "        pos_data[dp['ttcw_idx']].append(dp)\n",
    "    else:\n",
    "        neg_data[dp['ttcw_idx']].append(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty set found in pos_data for k = 7\n",
      "empty set found in pos_data for k = 8\n",
      "empty set found in pos_data for k = 10\n",
      "empty set found in pos_data for k = 14\n"
     ]
    }
   ],
   "source": [
    "for k, l in pos_data.items():\n",
    "    if len(l) == 0:\n",
    "        print('empty set found in pos_data for k =', k)\n",
    "\n",
    "for k, l in neg_data.items():\n",
    "    if len(l) == 0:\n",
    "        print('empty set found in neg_data for k =', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_list = [7, 8, 10, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_demostrations = {k: pos_data[k][0] for k in pos_data if k not in hard_list}\n",
    "neg_demostrations = {k: neg_data[k][0] for k in neg_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'story_id': '2_GPT4', 'ttcw_idx': 1, 'binary_verdict': 'Yes'},\n",
       " 2: {'story_id': '0_GPT4', 'ttcw_idx': 2, 'binary_verdict': 'Yes'},\n",
       " 3: {'story_id': '0_GPT4', 'ttcw_idx': 3, 'binary_verdict': 'Yes'},\n",
       " 4: {'story_id': '0_GPT4', 'ttcw_idx': 4, 'binary_verdict': 'Yes'},\n",
       " 5: {'story_id': '1_GPT4', 'ttcw_idx': 5, 'binary_verdict': 'Yes'},\n",
       " 6: {'story_id': '4_GPT4', 'ttcw_idx': 6, 'binary_verdict': 'Yes'},\n",
       " 9: {'story_id': '0_GPT4', 'ttcw_idx': 9, 'binary_verdict': 'Yes'},\n",
       " 11: {'story_id': '11_GPT4', 'ttcw_idx': 11, 'binary_verdict': 'Yes'},\n",
       " 12: {'story_id': '10_GPT4', 'ttcw_idx': 12, 'binary_verdict': 'Yes'},\n",
       " 13: {'story_id': '0_GPT4', 'ttcw_idx': 13, 'binary_verdict': 'Yes'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_demostrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'story_id': '0_GPT3.5', 'ttcw_idx': 1, 'binary_verdict': 'No'},\n",
       " 2: {'story_id': '0_GPT3.5', 'ttcw_idx': 2, 'binary_verdict': 'No'},\n",
       " 3: {'story_id': '0_GPT3.5', 'ttcw_idx': 3, 'binary_verdict': 'No'},\n",
       " 4: {'story_id': '0_GPT3.5', 'ttcw_idx': 4, 'binary_verdict': 'No'},\n",
       " 5: {'story_id': '0_GPT3.5', 'ttcw_idx': 5, 'binary_verdict': 'No'},\n",
       " 6: {'story_id': '0_GPT3.5', 'ttcw_idx': 6, 'binary_verdict': 'No'},\n",
       " 7: {'story_id': '0_GPT3.5', 'ttcw_idx': 7, 'binary_verdict': 'No'},\n",
       " 8: {'story_id': '0_GPT3.5', 'ttcw_idx': 8, 'binary_verdict': 'No'},\n",
       " 9: {'story_id': '0_GPT3.5', 'ttcw_idx': 9, 'binary_verdict': 'No'},\n",
       " 10: {'story_id': '0_GPT3.5', 'ttcw_idx': 10, 'binary_verdict': 'No'},\n",
       " 11: {'story_id': '0_GPT3.5', 'ttcw_idx': 11, 'binary_verdict': 'No'},\n",
       " 12: {'story_id': '0_GPT3.5', 'ttcw_idx': 12, 'binary_verdict': 'No'},\n",
       " 13: {'story_id': '0_GPT3.5', 'ttcw_idx': 13, 'binary_verdict': 'No'},\n",
       " 14: {'story_id': '0_GPT3.5', 'ttcw_idx': 14, 'binary_verdict': 'No'}}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_demostrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Evaluator Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_model_comp(run_id, likert = False ):\n",
    "    ground_truth = load_json(curr_dir + 'data/output/{}/inference_output.json'.format(run_id))\n",
    "    eval_predict = load_json(curr_dir + 'data/output/{}/eval_output_cleaned.json'.format(run_id))\n",
    "\n",
    "    all_comp_data = []\n",
    "    for story_key in ground_truth:\n",
    "        model_output = {k: -1 for k in range(1, 15)}\n",
    "        found_match = False\n",
    "        for pred_dp in eval_predict:\n",
    "            if pred_dp['prompt_id'][0] == story_key:\n",
    "                model_output[pred_dp['prompt_id'][1]] = pred_dp['cleaned_output']\n",
    "                found_match = True\n",
    "        if found_match:\n",
    "            all_comp_data.append({\n",
    "                'story_id': story_key,\n",
    "                'human_output': ground_truth[story_key]['human_output'],\n",
    "                'model_output': model_output\n",
    "            })\n",
    "    \n",
    "\n",
    "    acc_by_q = {k: 0 for k in range(1, 15)}\n",
    "    pos_true = 0\n",
    "    pos_pred = 0\n",
    "    invalid_pred = 0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    \n",
    "    for k in range(1, 15):\n",
    "        true = [comp['human_output'][str(k)] for comp in all_comp_data]\n",
    "        pred = [comp['model_output'][k] for comp in all_comp_data]\n",
    "        invalid_pred += sum([p == -1 for p in pred])\n",
    "        all_true.extend(true)\n",
    "        all_pred.extend(pred)\n",
    "        if likert:\n",
    "            acc_by_q[k] = round(pearsonr(true, pred).statistic, 2)\n",
    "        else:\n",
    "            acc_by_q[k] = round(accuracy_score(true, pred), 2)\n",
    "        pos_true += sum(true)\n",
    "        pos_pred += sum(pred)\n",
    "    acc_by_q['avg_acc'] = round(np.mean(list(acc_by_q.values())), 2)\n",
    "    if likert:\n",
    "        acc_by_q['all_acc'] = round(pearsonr(all_true, all_pred).statistic, 2)\n",
    "        pos_pred = round(pos_pred / (5 * len(all_comp_data) * 14), 2)\n",
    "    else:\n",
    "        acc_by_q['all_acc'] = round(accuracy_score(all_true, all_pred), 2)\n",
    "    return acc_by_q, pos_true, pos_pred, invalid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_id = 'ttcw_evaluator_check/claude/5_scale_eval_ds_llama70b'\n",
    "# human_model_comp(run_id, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 0.17,\n",
       "  2: 0.5,\n",
       "  3: 0.58,\n",
       "  4: 0.67,\n",
       "  5: 0.17,\n",
       "  6: 0.17,\n",
       "  7: 0.42,\n",
       "  8: 0.5,\n",
       "  9: 0.67,\n",
       "  10: 1.0,\n",
       "  11: 0.42,\n",
       "  12: 0.0,\n",
       "  13: 0.67,\n",
       "  14: 0.17,\n",
       "  'avg_acc': 0.44,\n",
       "  'all_acc': 0.43},\n",
       " 39,\n",
       " 132,\n",
       " 0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = 'ttcw_evaluator_check/claude/few_shot_eval_gpt4o'\n",
    "human_model_comp(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_id = 'ttcw_evaluator_check/claude/5_scale_eval_gpt4o_mini'\n",
    "# human_model_comp(run_id, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 0.33,\n",
       "  2: 0.58,\n",
       "  3: 0.58,\n",
       "  4: 0.5,\n",
       "  5: 0.17,\n",
       "  6: 0.25,\n",
       "  7: 0.42,\n",
       "  8: 0.25,\n",
       "  9: 0.67,\n",
       "  10: 0.42,\n",
       "  11: 0.42,\n",
       "  12: 0.08,\n",
       "  13: 0.67,\n",
       "  14: 0.33,\n",
       "  'avg_acc': np.float64(0.4),\n",
       "  'all_acc': 0.4},\n",
       " 39,\n",
       " 131,\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = 'ttcw_evaluator_check/claude/few_shot_eval_llama70b'\n",
    "human_model_comp(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 0.17,\n",
       "  2: 0.5,\n",
       "  3: 0.58,\n",
       "  4: 0.58,\n",
       "  5: 1.0,\n",
       "  6: 0.17,\n",
       "  7: 0.33,\n",
       "  8: 0.67,\n",
       "  9: 0.83,\n",
       "  10: 1.0,\n",
       "  11: 0.17,\n",
       "  12: 0.0,\n",
       "  13: 0.75,\n",
       "  14: 0.08,\n",
       "  'avg_acc': np.float64(0.49),\n",
       "  'all_acc': 0.49},\n",
       " 39,\n",
       " 113,\n",
       " 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = 'ttcw_evaluator_check/claude/few_shot_eval_qwen32b'\n",
    "human_model_comp(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 0.5,\n",
       "  2: 0.67,\n",
       "  3: 0.42,\n",
       "  4: 0.33,\n",
       "  5: 0.67,\n",
       "  6: 0.75,\n",
       "  7: 0.75,\n",
       "  8: 0.75,\n",
       "  9: 1.0,\n",
       "  10: 1.0,\n",
       "  11: 0.92,\n",
       "  12: 0.33,\n",
       "  13: 0.5,\n",
       "  14: 1.0,\n",
       "  'avg_acc': np.float64(0.68),\n",
       "  'all_acc': 0.68},\n",
       " 39,\n",
       " 28,\n",
       " 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = 'ttcw_evaluator_check/claude/few_shot_eval_qwen72b'\n",
    "human_model_comp(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def agg_pred(model_output, question, method = 'majority'):\n",
    "    all_pred = [model_output[model][question] for model in model_output]\n",
    "    if method == 'majority':\n",
    "        return Counter(all_pred).most_common(1)[0][0]\n",
    "    elif method == 'all':\n",
    "        return 1 if sum(all_pred) == len(all_pred) else 0\n",
    "    \n",
    "def get_metrics(true, pred):\n",
    "    # all_true = []\n",
    "    # all_pred = []\n",
    "    invalid_pred = sum([p == -1 for p in pred])\n",
    "    # all_true.extend(true)\n",
    "    # all_pred.extend(pred)\n",
    "    pearson_r = round(pearsonr(true, pred).statistic, 2)\n",
    "    pearson_p = round(pearsonr(true, pred).pvalue, 2)\n",
    "    acc = round(accuracy_score(true, pred), 2)\n",
    "    fi = round(f1_score(true, pred), 2)\n",
    "    pos_true = sum(true)\n",
    "    pos_pred = sum(pred)\n",
    "    return {\n",
    "        'pearson_r': pearson_r,\n",
    "        'pearson_p': pearson_p,\n",
    "        'acc': acc,\n",
    "        'fi': fi,\n",
    "        'pos_true': pos_true,\n",
    "        'pos_pred': pos_pred,\n",
    "        'invalid': invalid_pred,\n",
    "        'all_data': len(true)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([0, 1]).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter([1, 0]).most_common(0)#[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ihome/xli/joh227/developer/creative_bench/creative_bench/notebooks'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_model_comp_merged(run_id_lst, method = 'all', likert = False):\n",
    "    all_comp_data = {}\n",
    "    for run_id in run_id_lst:\n",
    "        ground_truth = load_json(curr_dir + 'data/output/{}/inference_output.json'.format(run_id))\n",
    "        eval_predict = load_json(curr_dir + 'data/output/{}/eval_output_cleaned.json'.format(run_id))\n",
    "        # print(len(ground_truth))\n",
    "        # print(len(eval_predict))\n",
    "        \n",
    "        for story_key in ground_truth:\n",
    "            model_output = {k: -1 for k in range(1, 15)}\n",
    "            found_match = False\n",
    "            for pred_dp in eval_predict:\n",
    "                if pred_dp['prompt_id'][0] == story_key:\n",
    "                    model_output[pred_dp['prompt_id'][1]] = pred_dp['cleaned_output']\n",
    "                    found_match = True\n",
    "            if found_match:\n",
    "                if story_key not in all_comp_data:\n",
    "                    all_comp_data[story_key] = ({\n",
    "                        'human_output': ground_truth[story_key]['human_output'],\n",
    "                        'model_output': {\n",
    "                            run_id.split('/')[-1].split('_')[-1]: model_output\n",
    "                        }\n",
    "                    })\n",
    "                else:\n",
    "                    all_comp_data[story_key]['model_output'][run_id.split('/')[-1].split('_')[-1]] = model_output\n",
    "    \n",
    "\n",
    "    result_data = []\n",
    "    for k in [1, 2, 6, 13]:\n",
    "        true = [all_comp_data[comp]['human_output'][str(k)] for comp in all_comp_data]\n",
    "        pred = [\n",
    "            agg_pred(all_comp_data[comp]['model_output'], k, method)\n",
    "            for comp in all_comp_data\n",
    "        ]\n",
    "        metrics = get_metrics(true, pred)\n",
    "        metrics['ttcw_idx'] = k\n",
    "        result_data.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearson_r</th>\n",
       "      <th>pearson_p</th>\n",
       "      <th>acc</th>\n",
       "      <th>fi</th>\n",
       "      <th>pos_true</th>\n",
       "      <th>pos_pred</th>\n",
       "      <th>invalid</th>\n",
       "      <th>all_data</th>\n",
       "      <th>ttcw_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.29</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.35</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.38</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pearson_r  pearson_p   acc    fi  pos_true  pos_pred  invalid  all_data  \\\n",
       "2       0.21       0.21  0.86  0.29         4         3        0        36   \n",
       "0       0.29       0.09  0.69  0.35         4        13        0        36   \n",
       "3       0.40       0.02  0.72  0.38        13         3        0        36   \n",
       "1       0.45       0.01  0.78  0.60        11         9        0        36   \n",
       "\n",
       "   ttcw_idx  \n",
       "2         6  \n",
       "0         1  \n",
       "3        13  \n",
       "1         2  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id_lst = [\n",
    "    'ttcw_evaluator_check/all',\n",
    "]\n",
    "human_model_comp_merged(run_id_lst, method = 'majority').sort_values(by = 'pearson_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearson_r</th>\n",
       "      <th>pearson_p</th>\n",
       "      <th>acc</th>\n",
       "      <th>fi</th>\n",
       "      <th>pos_true</th>\n",
       "      <th>pos_pred</th>\n",
       "      <th>invalid</th>\n",
       "      <th>all_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.60</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pearson_r  pearson_p   acc    fi  pos_true  pos_pred  invalid  all_data\n",
       "0       0.00       1.00  0.50  0.25         2         6        0        12\n",
       "1       0.35       0.26  0.67  0.60         6         4        0        12\n",
       "2       0.26       0.42  0.75  0.40         2         3        0        12\n",
       "3       0.32       0.32  0.50  0.40         8         2        0        12"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id_lst = [\n",
    "    'ttcw_evaluator_check/claude/few_shot_eval_qwen72b',\n",
    "    # 'ttcw_evaluator_check/claude/few_shot_eval_llama70b',\n",
    "    # 'ttcw_evaluator_check/claude/few_shot_eval_gpt4o_mini'\n",
    "]\n",
    "human_model_comp_merged(run_id_lst, method = 'majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_r = round(pearsonr(true, pred).statistic, 2)\n",
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:18: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_p = round(pearsonr(true, pred).pvalue, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearson_r</th>\n",
       "      <th>pearson_p</th>\n",
       "      <th>acc</th>\n",
       "      <th>fi</th>\n",
       "      <th>pos_true</th>\n",
       "      <th>pos_pred</th>\n",
       "      <th>invalid</th>\n",
       "      <th>all_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.67</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pearson_r  pearson_p   acc    fi  pos_true  pos_pred  invalid  all_data\n",
       "0       0.45       0.14  0.67  0.50         2         6        0        12\n",
       "1       0.48       0.12  0.75  0.67         4         5        0        12\n",
       "2        NaN        NaN  0.92  0.00         1         0        0        12\n",
       "3       0.43       0.17  0.75  0.40         4         1        0        12"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id_lst = [\n",
    "    'ttcw_evaluator_check/gpt4/few_shot_eval_qwen72b',\n",
    "    # 'ttcw_evaluator_check/gpt4/few_shot_eval_llama70b',\n",
    "    # 'ttcw_evaluator_check/gpt4/few_shot_eval_gpt4o_mini'\n",
    "]\n",
    "human_model_comp_merged(run_id_lst, method = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_r = round(pearsonr(true, pred).statistic, 2)\n",
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:18: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_p = round(pearsonr(true, pred).pvalue, 2)\n",
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_r = round(pearsonr(true, pred).statistic, 2)\n",
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:18: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_p = round(pearsonr(true, pred).pvalue, 2)\n",
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_r = round(pearsonr(true, pred).statistic, 2)\n",
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:18: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_p = round(pearsonr(true, pred).pvalue, 2)\n",
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:17: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_r = round(pearsonr(true, pred).statistic, 2)\n",
      "/scratch/slurm-1117283/ipykernel_2884/159919049.py:18: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pearson_p = round(pearsonr(true, pred).pvalue, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearson_r</th>\n",
       "      <th>pearson_p</th>\n",
       "      <th>acc</th>\n",
       "      <th>fi</th>\n",
       "      <th>pos_true</th>\n",
       "      <th>pos_pred</th>\n",
       "      <th>invalid</th>\n",
       "      <th>all_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pearson_r  pearson_p   acc   fi  pos_true  pos_pred  invalid  all_data\n",
       "0        NaN        NaN  0.92  0.0         0         1        0        12\n",
       "1        NaN        NaN  0.92  0.0         1         0        0        12\n",
       "2        NaN        NaN  0.92  0.0         1         0        0        12\n",
       "3        NaN        NaN  0.92  0.0         1         0        0        12"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id_lst = [\n",
    "    'ttcw_evaluator_check/gpt3_5/few_shot_eval_qwen72b',\n",
    "    # 'ttcw_evaluator_check/gpt3_5/few_shot_eval_llama70b',\n",
    "    # 'ttcw_evaluator_check/claude/few_shot_eval_gpt4o_mini'\n",
    "]\n",
    "human_model_comp_merged(run_id_lst, method = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a combined json for qwen72b inference output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`x` and `y` must have length at least 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/scratch/slurm-1118776/ipykernel_29672/836749537.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'ttcw_evaluator_check/all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# 'ttcw_evaluator_check/claude/few_shot_eval_llama70b',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 'ttcw_evaluator_check/claude/few_shot_eval_gpt4o_mini'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhuman_model_comp_merged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'majority'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/slurm-1118776/ipykernel_29672/3231463366.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(run_id_lst, method, likert)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_comp_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         result_data.append(\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/slurm-1118776/ipykernel_29672/159919049.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(true, pred)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# all_pred = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0minvalid_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# all_true.extend(true)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# all_pred.extend(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpearson_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mpearson_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/creative_bench/lib/python3.12/site-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, alternative, method, axis)\u001b[0m\n\u001b[1;32m   4548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4549\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`x` and `y` must have the same length along `axis`.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4552\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`x` and `y` must have length at least 2.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4554\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4555\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `x` and `y` must have length at least 2."
     ]
    }
   ],
   "source": [
    "run_id_lst = [\n",
    "    'ttcw_evaluator_check/all',\n",
    "    # 'ttcw_evaluator_check/claude/few_shot_eval_llama70b',\n",
    "    # 'ttcw_evaluator_check/claude/few_shot_eval_gpt4o_mini'\n",
    "]\n",
    "human_model_comp_merged(run_id_lst, method = 'majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olmo_7b',\n",
       " 'mistral_small_24b',\n",
       " 'deepseek_qwen_32b',\n",
       " 'mixtral_8x7b',\n",
       " 'deepseek_qwen_7b',\n",
       " 'llama3_8b_instr',\n",
       " 'gemini_2_flash',\n",
       " 'olmo_13b_sft',\n",
       " 'qwen_32b_instruct',\n",
       " 'gpt_4.1_mini',\n",
       " 'deepseek_r1',\n",
       " 'olmo_13b_dpo',\n",
       " 'deepseek_llama_70b',\n",
       " 'mistral_7b_instr',\n",
       " 'olmo_13b',\n",
       " 'deepseek_v3',\n",
       " 'gpt_4.1',\n",
       " 'qwen_32b_instruct_1',\n",
       " 'llama3_70b_instruct',\n",
       " 'claude_37_sonnet',\n",
       " 'qwen_7b_instruct',\n",
       " 'claude_3_haiku',\n",
       " 'qwen_72b_instruct']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_dir = \"/ix1/xli/bkb45/joey_files/creative_bench\"\n",
    "\n",
    "batch_id = 'ttcw_tmp_var'\n",
    "batch_id = 'ttcw_v1_0'\n",
    "\n",
    "exclude_lst = [\n",
    "    'gemini_2_pro',\n",
    "    '__MACOSX'\n",
    "]\n",
    "\n",
    "output_dir = '{}/{}/{}/'.format(proj_dir, \"data/output\", batch_id)\n",
    "# output_dir = '../results/'\n",
    "model_lst = [m for m in os.listdir(output_dir) if os.path.isdir(output_dir + m) and m not in exclude_lst]\n",
    "\n",
    "model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output_dir\n",
    "# # model_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# skip_lst = [] #['qwen_32b_instruct']\n",
    "# for model in model_lst:\n",
    "#     if model in skip_lst: continue\n",
    "#     inference_results = load_json(\n",
    "#         '/ix1/xli/bkb45/joey_files/creative_bench/data/output/{}/{}/inference_output.json'.format(\n",
    "#             batch_id, model\n",
    "#         )\n",
    "#     )\n",
    "#     print(len(inference_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_dfs = []\n",
    "skip_lst = []\n",
    "skip_lst = ['qwen_32b_instruct'] # we are using qwen_32b_instruct_1 instead\n",
    "for model in model_lst:\n",
    "    if model in skip_lst: continue\n",
    "    tmp_df = pd.read_csv(output_dir + model + '/eval_report.csv')\n",
    "    tmp_df['model'] = model\n",
    "    result_dfs.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df = pd.concat(result_dfs)\n",
    "combined_df['passed'] = combined_df.apply(\n",
    "    lambda row: 1 if sum([\n",
    "        row[col] for col in combined_df.columns if '-' in col\n",
    "    ]) == 4 else 0,\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fluency - Narrative Ending</th>\n",
       "      <th>Fluency - Understandability and Coherence</th>\n",
       "      <th>Flexibility - Emotional Flexibility</th>\n",
       "      <th>Elaboration - World Building and Setting</th>\n",
       "      <th>passed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claude_37_sonnet</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4.1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_v3</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4.1_mini</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_r1</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_llama_70b</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>olmo_13b</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_qwen_7b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_3_haiku</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_qwen_32b</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3_70b_instruct</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3_8b_instr</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_7b_instr</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_2_flash</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_small_24b</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixtral_8x7b</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>olmo_13b_dpo</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>olmo_13b_sft</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>olmo_7b</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen_32b_instruct_1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen_72b_instruct</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen_7b_instruct</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Fluency - Narrative Ending  \\\n",
       "model                                             \n",
       "claude_37_sonnet                           0.75   \n",
       "gpt_4.1                                    1.00   \n",
       "deepseek_v3                                0.83   \n",
       "gpt_4.1_mini                               1.00   \n",
       "deepseek_r1                                0.83   \n",
       "deepseek_llama_70b                         0.50   \n",
       "olmo_13b                                   0.67   \n",
       "deepseek_qwen_7b                           0.00   \n",
       "claude_3_haiku                             0.33   \n",
       "deepseek_qwen_32b                          0.42   \n",
       "llama3_70b_instruct                        0.00   \n",
       "llama3_8b_instr                            0.00   \n",
       "mistral_7b_instr                           0.17   \n",
       "gemini_2_flash                             0.83   \n",
       "mistral_small_24b                          0.25   \n",
       "mixtral_8x7b                               0.17   \n",
       "olmo_13b_dpo                               0.83   \n",
       "olmo_13b_sft                               0.42   \n",
       "olmo_7b                                    0.75   \n",
       "qwen_32b_instruct_1                        0.00   \n",
       "qwen_72b_instruct                          0.42   \n",
       "qwen_7b_instruct                           0.08   \n",
       "\n",
       "                     Fluency - Understandability and Coherence  \\\n",
       "model                                                            \n",
       "claude_37_sonnet                                          0.58   \n",
       "gpt_4.1                                                   0.67   \n",
       "deepseek_v3                                               0.50   \n",
       "gpt_4.1_mini                                              0.83   \n",
       "deepseek_r1                                               0.50   \n",
       "deepseek_llama_70b                                        0.50   \n",
       "olmo_13b                                                  0.33   \n",
       "deepseek_qwen_7b                                          0.00   \n",
       "claude_3_haiku                                            0.25   \n",
       "deepseek_qwen_32b                                         0.17   \n",
       "llama3_70b_instruct                                       0.33   \n",
       "llama3_8b_instr                                           0.08   \n",
       "mistral_7b_instr                                          0.08   \n",
       "gemini_2_flash                                            0.42   \n",
       "mistral_small_24b                                         0.25   \n",
       "mixtral_8x7b                                              0.08   \n",
       "olmo_13b_dpo                                              0.58   \n",
       "olmo_13b_sft                                              0.17   \n",
       "olmo_7b                                                   0.25   \n",
       "qwen_32b_instruct_1                                       0.17   \n",
       "qwen_72b_instruct                                         0.50   \n",
       "qwen_7b_instruct                                          0.17   \n",
       "\n",
       "                     Flexibility - Emotional Flexibility  \\\n",
       "model                                                      \n",
       "claude_37_sonnet                                    0.58   \n",
       "gpt_4.1                                             0.83   \n",
       "deepseek_v3                                         0.67   \n",
       "gpt_4.1_mini                                        0.42   \n",
       "deepseek_r1                                         0.58   \n",
       "deepseek_llama_70b                                  0.17   \n",
       "olmo_13b                                            0.25   \n",
       "deepseek_qwen_7b                                    0.00   \n",
       "claude_3_haiku                                      0.00   \n",
       "deepseek_qwen_32b                                   0.25   \n",
       "llama3_70b_instruct                                 0.00   \n",
       "llama3_8b_instr                                     0.08   \n",
       "mistral_7b_instr                                    0.00   \n",
       "gemini_2_flash                                      0.58   \n",
       "mistral_small_24b                                   0.00   \n",
       "mixtral_8x7b                                        0.00   \n",
       "olmo_13b_dpo                                        0.25   \n",
       "olmo_13b_sft                                        0.42   \n",
       "olmo_7b                                             0.17   \n",
       "qwen_32b_instruct_1                                 0.00   \n",
       "qwen_72b_instruct                                   0.08   \n",
       "qwen_7b_instruct                                    0.00   \n",
       "\n",
       "                     Elaboration - World Building and Setting  passed  \n",
       "model                                                                  \n",
       "claude_37_sonnet                                         0.42    0.33  \n",
       "gpt_4.1                                                  0.50    0.33  \n",
       "deepseek_v3                                              0.50    0.17  \n",
       "gpt_4.1_mini                                             0.50    0.17  \n",
       "deepseek_r1                                              0.58    0.17  \n",
       "deepseek_llama_70b                                       0.33    0.08  \n",
       "olmo_13b                                                 0.08    0.08  \n",
       "deepseek_qwen_7b                                         0.00    0.00  \n",
       "claude_3_haiku                                           0.08    0.00  \n",
       "deepseek_qwen_32b                                        0.17    0.00  \n",
       "llama3_70b_instruct                                      0.00    0.00  \n",
       "llama3_8b_instr                                          0.00    0.00  \n",
       "mistral_7b_instr                                         0.00    0.00  \n",
       "gemini_2_flash                                           0.17    0.00  \n",
       "mistral_small_24b                                        0.00    0.00  \n",
       "mixtral_8x7b                                             0.00    0.00  \n",
       "olmo_13b_dpo                                             0.08    0.00  \n",
       "olmo_13b_sft                                             0.08    0.00  \n",
       "olmo_7b                                                  0.08    0.00  \n",
       "qwen_32b_instruct_1                                      0.00    0.00  \n",
       "qwen_72b_instruct                                        0.17    0.00  \n",
       "qwen_7b_instruct                                         0.00    0.00  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df = combined_df.groupby('model').agg('sum')\n",
    "grouped_df = grouped_df.drop(columns = ['dp_id'])\n",
    "grouped_df = (grouped_df / result_dfs[0].shape[0]).round(2).sort_values(by = 'passed', ascending = False)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_df.to_csv('../results/summary_{}.csv'.format(batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
