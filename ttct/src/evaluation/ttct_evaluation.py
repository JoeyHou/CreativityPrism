"""Use LLM-as-a-judge to evaluate creative responses generated by LLMs during inference
"""
import argparse
import logging
from vllm import LLM, SamplingParams
from src.utils.helpers import load_json, save_json

EVAL_PROMPT = '''You are an expert of psychology. Your objective is to assess the subject’s creativity through their answers to some question/answering task related to divergent thinking.
You will be given a question-answer pair. Your task is to score the answer.
You should rate the answer on five metrics. For all five metrics, assign a score between 1 and 5, with 5 being the highest. Five metrics are:
1. Fluency. Fluency refers to the ability to generate a large quantity of ideas or solutions to a given problem. This measure isn’t concerned with the quality or uniqueness of the ideas, but rather the sheer volume. The more ideas one can produce, the higher the fluency.
2. Flexibility. Flexibility is the capacity to shift one’s thinking and to produce a wide range of ideas from different categories or perspectives. It involves being able to think outside of the box and to switch from one type of idea to another.
3. Originality. Originality refers to the ability to come up with unique or novel ideas that differ from the norm. It’s not just about producing many ideas (fluency), but also about producing ideas that are different from what others might typically think of.
4. Elaboration. Elaboration is the ability to expand upon or add detail to ideas. It involves taking a simple idea and building upon it, adding complexity and depth. Elaboration isn’t just about creating more, but about deepening what is there.
5. Finally, you will provide an overall score between 1 and 5, with 5 being the highest.
You should only give the score, format like:
Fluency: 3 
Question:'''

def main(infer_model_name, eval_model_name, temp, cache_dir):
    logger = logging.getLogger(__name__)

    # Load creative questions from original paper
    data_path = f'data/outputs/temp_{temp}/{infer_model_name}.json'
    data = load_json(data_path)

    # Load model using 4 GPUs
    llm = LLM(
            model=f'{cache_dir}/{eval_model_name}', 
            tensor_parallel_size=4,        
            gpu_memory_utilization=0.9, 
            max_model_len=2048
            )

    basic_questions = [item['input'][f'text_basic'] for item in data]
    basic_predictions = [item['output'][f'text_basic'] for item in data]
    basic_inputs = [f'''{EVAL_PROMPT} {quest} Answer: {pred}\n\nANSWER RATINGS ON FIVE METRICS: ''' 
                    for quest,pred in zip(basic_questions,basic_predictions)
                    ]
    
    instructive_questions = [item['input'][f'text_instructive'] for item in data]
    instructive_predictions = [item['output'][f'text_instructive'] for item in data]
    instructive_inputs = [f'''{EVAL_PROMPT} {quest} Answer: {pred}\n\nANSWER RATINGS ON FIVE METRICS: ''' 
                        for quest,pred in zip(instructive_questions,instructive_predictions)
                        ]

    cot_questions = [item['input'][f'text_cot'] for item in data]
    cot_predictions = [item['output'][f'text_cot'] for item in data]
    cot_inputs = [f'''{EVAL_PROMPT} {quest} Answer: {pred}\n\nANSWER RATINGS ON FIVE METRICS: ''' 
                    for quest,pred in zip(cot_questions,cot_predictions)
                    ]
    
    # Use hyperparameters from original paper, section 4.1.2 (temperature is variable for additional experiments)
    sampling_params = SamplingParams(max_tokens=512, 
                                temperature=temp, 
                                top_p=1, 
                                top_k=50)

    logger.info(f'Running evaluations on basic type prompts')
    basic_eval_pred = llm.generate(basic_inputs, sampling_params)
    basic_eval_pred = [output.outputs[0].text for output in basic_eval_pred]

    logger.info(f'Running evaluations on instructive type prompts')
    instructive_eval_pred = llm.generate(instructive_inputs, sampling_params)
    instructive_eval_pred = [output.outputs[0].text for output in instructive_eval_pred]

    logger.info(f'Running evaluations on cot type prompts')
    cot_eval_pred = llm.generate(cot_inputs, sampling_params)
    cot_eval_pred = [output.outputs[0].text for output in cot_eval_pred]

    for i, item in enumerate(data):
        item["evaluation"] = {
            f"text_basic": basic_eval_pred[i],
            f"text_instructive": instructive_eval_pred[i],
            f"text_cot": cot_eval_pred[i],
        }


    # Save the updated data to a JSON file
    eval_file_name = f'data/evaluations/temp_{temp}/{infer_model_name}.json'
    save_json(data, eval_file_name)
    logger.info(f"Evaluation for {infer_model_name} completed using {eval_model_name}.")


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='Conduct evaluation on creative outputs from LLMs (using LLM-as-a-judge).')

    parser.add_argument('-infer_model_name', 
                        type=str, 
                        default='Qwen2.5-72B-Instruct', 
                        help=f'Name of LLM being evaluated (should match name of local directory where weights are stored)'
                        )
    
    parser.add_argument('-eval_model_name', 
                        type=str, 
                        default='Qwen2.5-72B-Instruct', 
                        help=f'Name of LLM used to evaluate (should match name of local directory where weights are stored)'
                        )
    
    parser.add_argument('-temp', 
                        type=int, 
                        default=1, 
                        help=f'Original temp=1; however, it is variable here for additional experiments'
                        )
    
    parser.add_argument('-cache_dir', 
                        type=str, 
                        default='/playpen-ssd/pretrained_models', 
                        help=f'Path to local directory containing model weights'
                        )

    args = parser.parse_args()
    main(**vars(args)) 